### Отчет по первой лабораторной работе

---

## **1. Теоретическая база**

### **Цель работы**  
Разработать и реализовать архитектуру нейронной сети на основе **SqueezeNet** для задачи классификации изображений автомобилей из набора данных. Провести сравнение двух методов оптимизации: классического **Adam** и его варианта **AmsGrad**, оценив их влияние на точность модели и скорость сходимости.

### **Описание SqueezeNet**  
SqueezeNet — компактная сверточная нейронная сеть (CNN), которая достигает высокой точности классификации при минимальном количестве параметров. Основные компоненты:
- **Fire-модули:** содержат слои "squeeze" (1x1 свертки) и "expand" (1x1 и 3x3 свертки).
- **Уменьшение количества параметров:** замена сверток 3x3 на 1x1.
- **Позднее объединение:** Pooling слои перемещены ближе к выходу для сохранения информации.
- **Сжатие параметров:** достигается за счет агрессивной параметрической регуляризации.

### **Описание оптимизаторов**

#### **Adam**  
Adam (Adaptive Moment Estimation) адаптирует скорость обучения для каждого параметра на основе скользящих средних значений градиентов и их квадратов:
- Обновление скользящих средних градиента \( m_t \) и квадратов градиента \( v_t \):
  ```math
  m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
  v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
  ```
- Корректировка смещений:
  ```math
  \hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
  ```
- Обновление параметров:
  ```math
  \theta_t = \theta_{t-1} - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
  ```

#### **AmsGrad**  
AmsGrad модифицирует Adam, чтобы избежать проблем с медленной сходимостью, используя максимальное значение исторических градиентов:
```math
\hat{v}_t = \max(\hat{v}_{t-1}, v_t)
\theta_t = \theta_{t-1} - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
```

---

## **2. Разработанная система**

### **Архитектура SqueezeNet**
- **Fire-модули:** реализация блоков "squeeze" и "expand".
- **Сверточные слои:** извлечение признаков изображений.
- **Пулинг-слои:** уменьшение размерности признаков.
- **Полносвязный классификатор:** классификация изображений на классы.

### **Оптимизаторы**
- **Adam:** реализован через `torch.optim.Adam`.
- **AmsGrad:** настройка через параметр `amsgrad=True`.

### **Предобработка данных**
- Изображения нормализуются и масштабируются до \(224 \times 224\).
- Используемые преобразования:
  - `Resize`
  - `ToTensor`
  - `Normalize`

### **Процесс обучения**
- **Функция потерь:** `CrossEntropyLoss`.
- **Метрики:** потери и точность на обучающей и тестовой выборках.

---

## **3. Результаты выполнения программы**

### **Обучение с Adam**
**Тестовая точность:** 37.77%

![Снимок экрана 2025-01-13 004327](https://github.com/user-attachments/assets/5639cf30-ee30-446f-9937-4adf753a03d6)
![Снимок экрана 2025-01-13 004337](https://github.com/user-attachments/assets/3a909af0-b9a3-4693-a973-b7e14caaee1e)

### **Обучение с AmsGrad**
**Тестовая точность:** 54.20%

![Снимок экрана 2025-01-13 004346](https://github.com/user-attachments/assets/7762eec1-d01a-4cb3-8a3a-78f9c6634a1c)


---

## **4. Анализ результатов**
- **Сходимость:** AmsGrad показал более стабильное обновление градиентов и устойчивость к локальным минимумам.
- **Точность:** Adam на начальных этапах сходился быстрее, но AmsGrad обеспечил лучшую стабильность.
- **Время:** Adam оказался быстрее благодаря меньшему числу вычислений.

---

## **5. Использованные источники**
1. SqueezeNet: https://arxiv.org/abs/1602.07360  
2. AmsGrad Optimization: https://openreview.net/pdf?id=ryQu7f-RZ  
3. Stanford Cars Dataset: http://ai.stanford.edu/~jkrause/cars/car_dataset.html  
4. PyTorch Documentation: https://pytorch.org/docs/stable
